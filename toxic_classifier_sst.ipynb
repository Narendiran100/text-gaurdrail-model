{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narendiran100/text-gaurdrail-model/blob/master/toxic_classifier_sst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1pIAX3O98_2"
      },
      "source": [
        "# Toxic Text Classification using DistilBERT SST-2\n",
        "\n",
        "This notebook implements a text classifier to identify toxic content using the DistilBERT model fine-tuned on SST-2 dataset. The model is further fine-tuned on the Jigsaw Toxic Comment Classification dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UAw1Px-98_5"
      },
      "source": [
        "## 1. Setup and Dependencies\n",
        "\n",
        "Install required packages and configure the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4dJlj6p98_5"
      },
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets torch codecarbon pandas numpy matplotlib seaborn"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix1bTyoH98_6"
      },
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from codecarbon import EmissionsTracker\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for saving model checkpoints\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directories\n",
        "!mkdir -p '/content/drive/MyDrive/toxic_classifier_sst/models'\n",
        "!mkdir -p '/content/drive/MyDrive/toxic_classifier_sst/logs'"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2ThF0i98_6"
      },
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "Load the Jigsaw dataset and prepare it for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVcg2rYu98_7"
      },
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"thesofakillers/jigsaw-toxic-comment-classification-challenge\")\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    \"\"\"Convert multi-label toxic classification to binary labels.\n",
        "\n",
        "    Args:\n",
        "        examples: Dataset examples containing comment_text and toxicity labels\n",
        "\n",
        "    Returns:\n",
        "        dict: Processed examples with text and binary labels\n",
        "    \"\"\"\n",
        "    # Define toxicity types\n",
        "    toxicity_types = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "    # Create binary labels (1 for toxic, 0 for safe)\n",
        "    labels = []\n",
        "    for i in range(len(examples['comment_text'])):\n",
        "        is_toxic = any(examples[col][i] == 1 for col in toxicity_types)\n",
        "        labels.append(1 if is_toxic else 0)\n",
        "\n",
        "    return {\n",
        "        'text': examples['comment_text'],\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Split dataset\n",
        "train_test = dataset['train'].train_test_split(test_size=0.2)\n",
        "train_val = train_test['train'].train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = train_val['train']\n",
        "val_dataset = train_val['test']\n",
        "test_dataset = train_test['test']\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGydUYbA98_7"
      },
      "source": [
        "## 3. Model Setup and Training\n",
        "\n",
        "Initialize the SST-2 model and configure training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr1ryrSU98_7"
      },
      "source": [
        "# Initialize tokenizer and model\n",
        "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the input texts.\n",
        "\n",
        "    Args:\n",
        "        examples: Dataset examples containing text field\n",
        "\n",
        "    Returns:\n",
        "        dict: Tokenized inputs\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"Calculate evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        pred: Prediction object containing predictions and label_ids\n",
        "\n",
        "    Returns:\n",
        "        dict: Computed metrics\n",
        "    \"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Set training arguments with CO2 tracking\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/toxic_classifier_sst/models',\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=16,  # Smaller batch size for Colab\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/drive/MyDrive/toxic_classifier_sst/logs',\n",
        "    logging_steps=100,\n",
        "    save_total_limit=3,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    eval_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_accumulation_steps=2,  # Gradient accumulation for larger effective batch size\n",
        "    report_to=[\"codecarbon\",\"wandb\"],  # Track CO2 emissions\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NOXwjIt98_8"
      },
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Start CO2 emissions tracker\n",
        "tracker = EmissionsTracker(project_name=\"toxic_classifier_sst\")\n",
        "tracker.start()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Stop tracking emissions\n",
        "emissions = tracker.stop()\n",
        "print(f\"Total CO2 emissions: {emissions} kg\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iACEycgT98_8"
      },
      "source": [
        "## 4. Model Evaluation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4_H9HrC98_8"
      },
      "source": [
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"\\nTest Results:\")\n",
        "for key, value in test_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "save_path = '/content/drive/MyDrive/toxic_classifier_sst/models/final_model'\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30NNTos298_9"
      },
      "source": [
        "## 5. Inference Examples and Integration Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXMcKpxv98_9"
      },
      "source": [
        "class TextGuardrail:\n",
        "    \"\"\"Text classification model for detecting toxic content.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        \"\"\"Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "        \"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def check_text(self, text):\n",
        "        \"\"\"Check if input text is toxic.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to classify\n",
        "\n",
        "        Returns:\n",
        "            dict: Classification results\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        label = \"unsafe\" if predictions[0][1] > 0.5 else \"safe\"\n",
        "        confidence = float(predictions[0][1] if label == \"unsafe\" else predictions[0][0])\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"confidence\": f\"{confidence:.2%}\"\n",
        "        }\n",
        "\n",
        "# Initialize guardrail with saved model\n",
        "guardrail = TextGuardrail(\"/content/drive/MyDrive/toxic_classifier_sst/models/final_model_sst\")\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"Hello, how are you today?\",\n",
        "    \"I really enjoyed the movie!\",\n",
        "    \"You're so stupid and worthless\",\n",
        "    \"Let's work together to solve this problem\",\n",
        "    \"I hate you\"\n",
        "]\n",
        "\n",
        "# Test the model\n",
        "for text in test_texts:\n",
        "    result = guardrail.check_text(text)\n",
        "    print(f\"\\nInput: {result['text']}\")\n",
        "    print(f\"Classification: {result['label']}\")\n",
        "    print(f\"Confidence: {result['confidence']}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZObz6Ll98_9"
      },
      "source": [
        "## Real-time Integration Example\n",
        "\n",
        "Here's how the model could be integrated into a real-time system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vumaeox_98_9"
      },
      "source": [
        "def process_prompt(prompt, guardrail, threshold=0.5):\n",
        "    \"\"\"Process a prompt through the toxicity filter.\n",
        "\n",
        "    Args:\n",
        "        prompt: User input prompt\n",
        "        guardrail: TextGuardrail instance\n",
        "        threshold: Confidence threshold for rejection\n",
        "\n",
        "    Returns:\n",
        "        tuple: (bool, str) - (is_safe, message)\n",
        "    \"\"\"\n",
        "    result = guardrail.check_text(prompt)\n",
        "    confidence = float(result['confidence'].strip('%')) / 100\n",
        "\n",
        "    if result['label'] == 'unsafe' and confidence > threshold:\n",
        "        return False, \"This prompt contains potentially harmful content and cannot be processed.\"\n",
        "    return True, \"Prompt is safe for processing.\"\n",
        "\n",
        "# Example usage in a prompt processing pipeline\n",
        "def prompt_pipeline(user_input):\n",
        "    \"\"\"Example prompt processing pipeline.\n",
        "\n",
        "    Args:\n",
        "        user_input: Raw user input\n",
        "\n",
        "    Returns:\n",
        "        str: Response message\n",
        "    \"\"\"\n",
        "    # Check prompt safety\n",
        "    is_safe, message = process_prompt(user_input, guardrail)\n",
        "\n",
        "    if not is_safe:\n",
        "        return message\n",
        "\n",
        "    # If safe, continue with normal processing\n",
        "    return \"Processing your safe prompt: \" + user_input\n",
        "\n",
        "# Test the pipeline\n",
        "test_prompts = [\n",
        "    \"Write a poem about spring flowers\",\n",
        "    \"You're all worthless and should die\",\n",
        "    \"Help me solve this math problem\",\n",
        "    \"I will find you and hurt you\",\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nInput: {prompt}\")\n",
        "    print(f\"Result: {prompt_pipeline(prompt)}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3aGjPbQ98_-"
      },
      "source": [
        "## Potential Extensions\n",
        "\n",
        "1. Real-time System Integration:\n",
        "   - Deploy as a REST API using FastAPI/Flask\n",
        "   - Implement caching for frequent prompts\n",
        "   - Add rate limiting and request queuing\n",
        "\n",
        "2. Performance Improvements:\n",
        "   - Model quantization for faster inference\n",
        "   - Batch processing for multiple prompts\n",
        "   - GPU acceleration in production\n",
        "\n",
        "3. Enhanced Features:\n",
        "   - Confidence threshold adjustment\n",
        "   - Multi-language support\n",
        "   - Toxicity category detection\n",
        "   - Feedback loop for continuous improvement\n",
        "\n",
        "4. Monitoring and Maintenance:\n",
        "   - Track false positives/negatives\n",
        "   - Monitor model drift\n",
        "   - Regular retraining with new data\n",
        "   - A/B testing for improvements"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}